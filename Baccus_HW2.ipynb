{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baccus.HW2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganBaccus/CptS-437/blob/main/Baccus_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Ly48iQZeu5"
      },
      "source": [
        "# **Homework Assignment #2**\n",
        "\n",
        "Assigned: February 1, 2021\n",
        "\n",
        "Due: February 19, 2021\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(10 points) Consider training set accuracy and test set accuracy curves\n",
        "plotted below as a function of the number of nodes in a decision tree.\n",
        "While this graph plots accuracy, we can also compute error as 1.0 - accuracy.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1ScPyMBFemm6dbgu1saUqSV3dJdUlwIdd)\n",
        "\n",
        "Can you suggest a way to determine the amount of overfit in the learned model\n",
        "based on these curves? Explain / justify your answer.\n",
        "\n",
        "Based on the curve in the graph, what size decision tree would you choose to use and why?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The amount by which the training data accuracy exceeds the test data accuracy is equal to the overfit. This can be calculated by subtracting the test data accuracy from the training data acccuracy.\n",
        "The highest accuracy on the test data is produced when the size of the tree is about 10. Therefore, I would choose to use a decision tree with 10 nodes.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(10 points) To demonstrate your understanding of k-nearest neighbors, construct a labeled dataset where the dimensionality is 1 and the leave-one-out cross-validation accuracy for 1-nearest neighbor is always 0. As a reminder, leave-one-out uses all of the training data except one instance for learning the model and uses the held-out instance for testing, repeating the process for each possible holdout point and averaging the results. Therefore, this describes a situation where the classifier always gets the prediction wrong.\n",
        "\n",
        "Answer:\n",
        "\n",
        "If the data points all appear on the same axis and alternate between positive and negative. Therefore the neighbors of each data point will be in the alternate class and the prediction will always be incorrect.\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(20 points) Consider training a perceptron using the datapoints in the table below, presented in this order.\n",
        "\n",
        "Instance | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8\n",
        "--- | --- | --- | --- | --- | --- | --- | --- | ---\n",
        "Label $y$ | +1 | -1 | +1 | -1 | +1 | -1 | +1 | +1\n",
        "Data $(x_1,x_2)$ | 10, 10 | 0, 0 | 8, 4 | 3, 3 | 4, 8 | 0.5, 0.5 | 4, 3 | 2, 5\n",
        "\n",
        "Given an initial set of weights $w = (1, 1)$ and bias $b=0$, show each step of the perceptron algorithm for the above sequence of instances over one epoch. This includes computation of the activation and adjustment of the weights after each instance.\n",
        "\n",
        "What is the accuracy of the perceptron after this first epoch?\n",
        "\n",
        "Will this perceptron eventually converge on a model with zero error for the training data? Why or why not?\n",
        "\n",
        "Answer:\n",
        "\n",
        "$x_1$ | $x_2$ | $y$ | $w_1$ | $w_2$ | $b$ | $a$ | $ya$ \n",
        "--- | --- | --- | --- | --- | --- | --- | --- \n",
        " 10 |10 |  1 |  1 |  1 | 0 | 20 | 20 > 0\n",
        "  0 | 0 | -1 |  1 |  1 | 0 | 0  | 0 <= 0\n",
        "  8 | 4 |  1 |  1 |  1 |-1 | 11 | 11 > 0\n",
        "  3 | 3 | -1 |  1 |  1 |-1 | 5  |-5 <= 0 \n",
        "  4 | 8 |  1 | -2 | -2 |-2 |-26 | -26 <= 0\n",
        " .5 |.5 | -1 |  2 |  6 | -1| 3  | -3 <= 0 \n",
        "  4 | 3 |  1 |1.5 |5.5 | -2| 20.5 | 20.5 > 0 \n",
        "  2 | 5 |  1 |1.5 |5.5 | -2| 28.5 | 28.5 > 0 \n",
        "\n",
        "\n",
        "The accuracy of the pereptron after one epoch is .5 or 50%.\n",
        "\n",
        "If you were to graph the data points on a standard graph, you could linearly separate the positive examples from the negative examples. This indicates that perceptron will eventually converge (requires more epochs). Additionally, if you input these data points into the perceptron algoithm provided in class, then you can see that it will converge after 4 epochs.\n",
        "\n",
        "---\n",
        "\n",
        "#4\n",
        "\n",
        "(80 points) In this programming task you will gain familiarity with k-nearest neighbor classification, the sklearn machine learning library, and working with a handwriting recognition dataset.\n",
        "\n",
        "For this program, compare the accuracy of four classifiers for correctly classifying the hand-written number from the digits dataset available as a sklearn library (see https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html for details). The classifiers are:\n",
        "\n",
        "- Decision tree (you can use the sklearn library for this)\n",
        "- K nearest neighbors with 5 neighbors (you can use the sklearn library for this)\n",
        "- Majority classifier (you can use the sklearn library for this)\n",
        "- Your own implementation of a KNN classifier (do not use the sklearn library for this). This classifier should compute Euclidean distance between pairs of points and take the number of neighbors to consider as a parameter.\n",
        "\n",
        "To report performance, randomly select 2/3 of the data points to use for training and 1/3 to use for testing. Repeat 3 times and report accuracy results averaged over the 3 trials. Compare accuracy results for the classifiers. For your KNN implementation, try different values for $k$ including 1, 3, 5, 7, and 9. Argue which value of $k$ you would choose and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZAmhBvZX8nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d36b01e-e1a6-4eb9-bfda-1e1267b58996"
      },
      "source": [
        "from math import sqrt\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "classifiers = [\n",
        "               (DecisionTreeClassifier(), \"Decision Tree\"),\n",
        "               (KNeighborsClassifier(n_neighbors=5), \"5 Nearest Neighbors\"),\n",
        "               (DummyClassifier(strategy='most_frequent'), \"Majority\")\n",
        "               ]\n",
        "\n",
        "def accuracy(yTest, labels):\n",
        "  testLength = len(yTest)\n",
        "  right = 0\n",
        "  for i in range(testLength):\n",
        "    if labels[i] == yTest[i]:\n",
        "      right += 1\n",
        "  return (float(right) / float(testLength))\n",
        "\n",
        "# Euclidean distance calculation\n",
        "def distance(x, y):\n",
        "  d = 0.0\n",
        "  for i in range(len(x)-1):\n",
        "    d += (x[i] - y[i])**2\n",
        "  return sqrt(d)\n",
        "\n",
        "def findNeighbors(train, test, n_neighbors):\n",
        "  distances = list()\n",
        "  trainLength = len(train)\n",
        "  for i in range(trainLength):\n",
        "    distToNeighbor = distance(train[i], test)\n",
        "    distances.append((i, distToNeighbor))\n",
        "  distances.sort(key=lambda x: x[1])\n",
        "  neighbors = list()\n",
        "  for i in range(n_neighbors):\n",
        "    neighbors.append(distances[i][0])\n",
        "  return neighbors\n",
        "\n",
        "def KNearestNeighbors(xTrain, xTest, yTrain, yTest, n_neighbors):\n",
        "  right = 0\n",
        "  nTest = len(yTest)\n",
        "  for i in range(nTest):\n",
        "    neighbors = findNeighbors(xTrain, xTest[i], n_neighbors)\n",
        "    output_values = yTrain[neighbors]\n",
        "    counts = np.bincount(output_values)\n",
        "    prediction = np.argmax(counts)\n",
        "    if prediction == yTest[i]:\n",
        "      right += 1\n",
        "  return float(right) / float(nTest)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  t0 = time.time()\n",
        "  digits = load_digits()\n",
        "\n",
        "  n = len(digits.images)\n",
        "  data = digits.images.reshape((n, -1))\n",
        "\n",
        "  for clf, name in classifiers:\n",
        "    print(\"Classifier:\", name)\n",
        "    results = []\n",
        "    for i in range(3):\n",
        "      xTrain, xTest, yTrain, yTest = \\\n",
        "        train_test_split(data, digits.target, test_size=0.33, random_state = i)\n",
        "      clf.fit(xTrain, yTrain)\n",
        "      labels=clf.predict(xTest)\n",
        "      results.append(accuracy(yTest, labels))\n",
        "    print(\"Accuracy: \", np.mean(results))\n",
        "   \n",
        "  print(\"Classifier: My K-Nearest Neighbors (k=1)\")\n",
        "  results = []\n",
        "  for i in range(3):\n",
        "    xTrain, xTest, yTrain, yTest = \\\n",
        "      train_test_split(data, digits.target, test_size=0.33, random_state = i)\n",
        "    knn_results = KNearestNeighbors(xTrain, xTest, yTrain, yTest, 3)\n",
        "    results.append(knn_results)\n",
        "  print(\"Accuracy: \", np.mean(results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier: Decision Tree\n",
            "Accuracy:  0.8428731762065095\n",
            "Classifier: 5 Nearest Neighbors\n",
            "Accuracy:  0.978675645342312\n",
            "Classifier: Majority\n",
            "Accuracy:  0.08585858585858586\n",
            "Classifier: My K-Nearest Neighbors (k=1)\n",
            "Accuracy:  0.9831649831649831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfV-HmqmX1cb"
      },
      "source": [
        "I would use k=3 because it poduces the highest accuracy rate."
      ]
    }
  ]
}