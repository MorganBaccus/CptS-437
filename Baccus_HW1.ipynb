{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baccus.HW1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganBaccus/CptS-437/blob/main/Baccus_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Z3f_sttGtY"
      },
      "source": [
        "# **Homework Assignment #1**\n",
        "\n",
        "Assigned: January 20, 2021\n",
        "\n",
        "Due: February 1, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of four questions that require a short answer and one that requires you to generate some Python code. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        "(20 points) Use information gain to build a decision tree that predicts the value of the class $Play$ based on the input features $Ace$, $Ten$, and $FirstMove$, using the training data provided below. Show each step of your calculations.\n",
        "\n",
        "Ace | Ten | FirstMove | Play\n",
        "--- | --- | --- | ---\n",
        "false | false | false | stand\n",
        "true | false | true | hit\n",
        "true | true | false | hit\n",
        "true | true | true | stand\n",
        "\n",
        "Is this tree optimal? In other words, does it yield zero classification error on the training data with minimal depth? Explain why or why not. If it is not optimal, draw the optimal tree as well. You can either include the tree(s) as a picture or describe using text.\n",
        "\n",
        "Answer:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1_G8HJ7Eil5qXSRaOfaON2LCnTDS_dG-h)\n",
        "![](https://drive.google.com/uc?id=1vp_JVb3dHy8TnUBl0SnVQOAP3SSId1r7)\n",
        "![](https://drive.google.com/uc?id=1wzPbmG7P7I7NTbH3380SLLRWkI1VoJNU)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#2.\n",
        "\n",
        "(10 points) Express the concept (Play=Hit) learned by all of your trees in Problem 1 as logical if-then rules.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Original tree:\n",
        "\n",
        "Play=Hit: (Ace= True and Ten=False) or (Ace=True and Ten=True and FirstMove=False)\n",
        "\n",
        "Optimal Tree:\n",
        "\n",
        "Play=Hit: (Ten=False and FirstMove=True) or (Ten=True and FirstMove=False)\n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(10 points) The decision tree algorithm we discussed in class is a greedy algorithm. What does this\n",
        "mean? What is the reason that most decision tree learning algorithms are greedy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Greedy algorithms make decisions that are optimal at each local decision point. Greedy algorithms only consider the optimal decision at the current point rather than if it is a globally-optimal decision. For example, in problem one the greedy decision put Ace as the root node. However, the globally-optimal solution was to have Ten as the root node since that produced a smaller tree. The reason that greedy algorithms are used more often is that there are fewer computations required to solve for the tree and are thus easier to implement on large data sets. The number of calculations that a globally-optimal tree requires are exponential to the size of the problem, while greedy algorithms only require calculations equal to a polynomial function using the number of data entries and features.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(10 points) Suppose you are testing a new algorithm on a data set consisting of 100 positive and 100 negative examples. You plan to use leave-one-out cross-validation and compare your algorithm to a baseline function, a simple majority classifier. With leave-one-out cross-validation, you train the algorithm on 199 data points and test it on 1 data point. You repeat the process 400 times, letting each point having a chance to represent the test set, and report the average of the classification accuracies. Given a set of training data, the majority classifier always outputs the class that is in the majority in the training set, regardless of the input. You expect the majority classifier to achieve about 50% classification accuracy, but to your surprise, it scores zero every time. Why?\n",
        "\n",
        "Answer:\n",
        "\n",
        "When you remove an example from the test data, the other class will become the majority and thus be the predicted result. For example, if a negative value is removed then there will now be 99 negative examples and 100 postives examples. Negative will be the minority class and positive will be the majority class. The majority classifier will then predict that the removed example is positive when it was not.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#5.\n",
        "\n",
        "(75 points) In this problem you are asked to write a Python program using Google Colab. We provide code below to construct a decision tree using the measures of entropy and gain discussed in class. You need to enhance this program in three ways.\n",
        "\n",
        "- First, note that the provided code assumes that the features are all discrete (there is a finite number of possible feature values) and not continuous. Modify the code to handle either discrete or continuous-valued features. In this case, we ask that you convert continuous-valued features to discrete features using equal-frequency binning, where the number of bins can be a parameter or hard-coded.\n",
        "\n",
        "- Second, use the dataset found at http://eecs.wsu.edu/~cook/ml/alldata.csv to test your model. This a comma-separated data file, one line per data point. The last entry on each line is the class value and the remaining entries represent the feature values. This data is a ``human activity recognition using smartphones'' dataset. The features represent statistical summaries of sensor data collected with a phone-based 3D accelerometer (measuring phone acceleration in X, Y, and Z directions) and a 3D gyroscope (measuring 3-axial angular velocity). The phone was worn by participants while they performed two activities: sit (-1) and stand (1). You will need to read in the dataset and store the data in a structure that the program can process. To test the model, randomly select a subset of points for training the model and randomly select a second subset of points for testing the model. For now, do not worry about whether the subsets overlap or not (in the future, not overlapping will be an important issue).\n",
        "\n",
        "- Third, generate a learning curve to show how the model performs on the data. You can use functions available from the matplotlib library (https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.plot.html) to generate the plot. The y axis of the plot should be classification accuracy, tested on a randomly-selected subset of 100 instances. The x axis of the plot should be the number of instances that was used for training. Create a minimum of 10 points for your learning curve plot with a broad range of x values (you can add more than 10 points to make the curve more complete).\n",
        "\n",
        "Finally, you may notice that the curve jumps around quite a bit. Smooth the curve by repeating the process at least 10 times and plotting the average of the results over the 10 trials.\n",
        "\n",
        "What insights does the learning curve provide on the learning process and the need for a large amount of training data?\n",
        "\n",
        "*Note that all of the code you write needs to be entirely your own, not copied from another existing program or using existing libraries that perform the specified functionality.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLWMwFDeE3nE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "8e650f1c-e511-4bb0-8014-510c0085dd06"
      },
      "source": [
        "# Decision tree learning\n",
        "#\n",
        "# Assumes discrete features. Examples may be inconsistent. Stopping condition for tree\n",
        "# generation is when all examples have the same class, or there are no more features\n",
        "# to split on (in which case, use the majority class). If a split yields no examples\n",
        "# for a particular feature value, then the classification is based on the parent's\n",
        "# majority class.\n",
        "\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "from random import sample\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, majClass):\n",
        "        self.split_feature = -1 # -1 indicates leaf node\n",
        "        self.children = {} # dictionary of {feature_value: child_tree_node}\n",
        "        self.majority_class = majClass\n",
        "        \n",
        "def build_tree(examples):\n",
        "    if not examples:\n",
        "        return None\n",
        "    # collect sets of values for each feature index, based on the examples\n",
        "    features = {}\n",
        "    for feature_index in range(len(examples[0]) - 1):\n",
        "        features[feature_index] = set([example[feature_index] for example in examples])\n",
        "    return build_tree_1(examples, features)\n",
        "    \n",
        "def build_tree_1(examples, features):\n",
        "    tree_node = TreeNode(majority_class(examples))\n",
        "    # if examples all have same class, then return leaf node predicting this class\n",
        "    if same_class(examples):\n",
        "        return tree_node\n",
        "    # if no more features to split on, then return leaf node predicting majority class\n",
        "    if not features:\n",
        "        return tree_node\n",
        "    # split on best feature and recursively generate children\n",
        "    best_feature_index = best_feature(features, examples)\n",
        "    tree_node.split_feature = best_feature_index\n",
        "    remaining_features = features.copy()\n",
        "    remaining_features.pop(best_feature_index)\n",
        "    for feature_value in features[best_feature_index]:\n",
        "        split_examples = filter_examples(examples, best_feature_index, feature_value)\n",
        "        tree_node.children[feature_value] = build_tree_1(split_examples, remaining_features)\n",
        "    return tree_node\n",
        "\n",
        "def majority_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return max(set(classes), key = classes.count)\n",
        "\n",
        "def same_class(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    return (len(set(classes)) == 1)\n",
        "\n",
        "def best_feature(features, examples):\n",
        "    # Return index of feature with lowest entropy after split\n",
        "    best_feature_index = -1\n",
        "    best_entropy = 2.0 # max entropy = 1.0\n",
        "    for feature_index in features:\n",
        "        se = split_entropy(feature_index, features, examples)\n",
        "        if se < best_entropy:\n",
        "            best_entropy = se\n",
        "            best_feature_index = feature_index\n",
        "    return best_feature_index\n",
        "\n",
        "def split_entropy(feature_index, features, examples):\n",
        "    # Return weighted sum of entropy of each subset of examples by feature value.\n",
        "    se = 0.0\n",
        "    for feature_value in features[feature_index]:\n",
        "        split_examples = filter_examples(examples, feature_index, feature_value)\n",
        "        se += (float(len(split_examples)) / float(len(examples))) * entropy(split_examples)\n",
        "    return se\n",
        "\n",
        "def entropy(examples):\n",
        "    classes = [example[-1] for example in examples]\n",
        "    classes_set = set(classes)\n",
        "    class_counts = [classes.count(c) for c in classes_set]\n",
        "    e = 0.0\n",
        "    class_sum = sum(class_counts)\n",
        "    for class_count in class_counts:\n",
        "        if class_count > 0:\n",
        "            class_frac = float(class_count) / float(class_sum)\n",
        "            e += (-1.0)* class_frac * math.log(class_frac, 2.0)\n",
        "    return e\n",
        "\n",
        "def filter_examples(examples, feature_index, feature_value):\n",
        "    # Return subset of examples with given value for given feature index.\n",
        "    return list(filter(lambda example: example[feature_index] == feature_value, examples))\n",
        "\n",
        "def print_tree(tree_node, feature_names, depth = 1):\n",
        "    indent_space = depth * \"  \"\n",
        "    if tree_node.split_feature == -1: # leaf node\n",
        "        print(indent_space + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "    else:\n",
        "        for feature_value in tree_node.children:\n",
        "            print(indent_space + feature_names[tree_node.split_feature] + \" == \" + feature_value)\n",
        "            child_node = tree_node.children[feature_value]\n",
        "            if child_node:\n",
        "                print_tree(child_node, feature_names, depth+1)\n",
        "            else:\n",
        "                # no child node for this value, so use majority class of parent (tree_node)\n",
        "                print(indent_space + \"  \" + feature_names[-1] + \": \" + tree_node.majority_class)\n",
        "\n",
        "def classify(tree_node, instance):\n",
        "    if tree_node.split_feature == -1:\n",
        "        return tree_node.majority_class\n",
        "    child_node = tree_node.children[instance[tree_node.split_feature]]\n",
        "    if child_node:\n",
        "        return classify(child_node, instance)\n",
        "    else:\n",
        "        return tree_node.majority_class\n",
        "\n",
        "def makeDiscrete(data):\n",
        "  midPoint = len(data) / 2\n",
        "  sortedValues = copy.copy(data) # create a copy of the data to sort\n",
        "  sortedValues.sort()\n",
        "  threshold = sortedValues[int(midPoint)]\n",
        "\n",
        "  binnedData = np.zeros(len(data), dtype=int)\n",
        "\n",
        "  for i in range(len(data)): # iterate through data and bin based on if value is > or <= threshold\n",
        "    if (data[i] <= threshold):\n",
        "      binnedData[i]=0 # assign 0 if value is <= threshold\n",
        "    else:\n",
        "        binnedData[i]=1 # assign 1 if value is > threshold\n",
        "  return binnedData\n",
        "\n",
        "def plotGraph(x, y):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(x, y)\n",
        "  ax.set(xlabel='Training Size', ylabel='Accuracy')\n",
        "  plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   random.seed()\n",
        "   dataFrame = pd.read_csv('https://eecs.wsu.edu/~cook/ml/alldata.csv')\n",
        "   data = dataFrame.to_numpy()\n",
        "\n",
        "   if True: # check that the data is continuous\n",
        "      examples = np.zeros(np.shape(data), dtype=str)\n",
        "      for i in range(len(data[0])-1):\n",
        "         examples[:,i] = makeDiscrete(data[:,i])\n",
        "   else:\n",
        "      examples = data\n",
        "\n",
        "   for i in range(len(data)):\n",
        "     examples[i][len(data[0])-1] = str(data[i][len(data[0])-1])\n",
        "   \n",
        "   sampleData = []\n",
        "   output = []\n",
        "   numberOfTrials = 10\n",
        "\n",
        "   for trials in range(numberOfTrials):\n",
        "      count = 0\n",
        "\n",
        "      for i in range(1, 1000, 50):\n",
        "         testSet = sample(examples, 100)\n",
        "         trainSet = sample(examples, i)\n",
        "         tree = build_tree(trainSet)\n",
        "         right = 0\n",
        "\n",
        "         for j in range(100):\n",
        "            testClass = classify(tree, testSet[j])\n",
        "            if testClass == testSet[j][len(data[0])-1]:\n",
        "               right += 1\n",
        "         if trials == 0:\n",
        "            sampleData.append(i)\n",
        "            output.append(right)\n",
        "         else:\n",
        "            output[count] += right\n",
        "         count += 1\n",
        "\n",
        "   for i in range(len(output)):\n",
        "      output[i] = output[i] / float(numberOfTrials)\n",
        "      \n",
        "   plotGraph(sampleData, output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MybXjx2MtGDx"
      },
      "source": [
        "If there are too few data points then there is a risk of underfitting the graph. However, if there is too much training data then there is a risk of overfitting the graph."
      ]
    }
  ]
}