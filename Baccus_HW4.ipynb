{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baccus.HW4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganBaccus/CptS-437/blob/main/Baccus_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsLGOMPRM8xK"
      },
      "source": [
        "# **Homework Assignment #4**\n",
        "\n",
        "Assigned: March 5, 2021\n",
        "\n",
        "Due: March 24, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for your notebook as your homework submission.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (10 points) Consider the subset of (x,y) points shown below. These are actually a subset of the data points found in the sklearn diabetes dataset.\n",
        "\n",
        "x | y\n",
        "--- | ---\n",
        " 0.08 | 233\n",
        "-0.04 | 91\n",
        " 0.01 | 111\n",
        "-0.04 | 152\n",
        "-0.03 | 120\n",
        " 0.01 | 67\n",
        " 0.09 | 310\n",
        "-0.03 | 94\n",
        "-0.06 | 183\n",
        "-0.03 | 66\n",
        " 0.06 | 173\n",
        "-0.06 | 72\n",
        " 0.00 | 49\n",
        "-0.02 | 64\n",
        "-0.07 | 48\n",
        "\n",
        "\n",
        "For a candidate linear regressor with parameters $\\Theta_0 = 75.1$ and $\\Theta_1 = -0.001$, calculate the mean squared error with respect to the data points and perform one iteration of gradient descent , assuming $\\alpha = 0.01$. Show all of your work.\n",
        "\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "JY6Xhur83gG0",
        "outputId": "6bd746e3-bf94-4573-bfe6-92ac9d92b83a"
      },
      "source": [
        "\"\"\"\n",
        "This is the example from class that shows how to use Linear Regression to decrease error\n",
        "-Using hard coded data\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        "x = [.08, -.04, .01, -.04, -.03, .01, .09, -.03, -.06, -.03, .06, -.05, 0, -.2, -.7]\n",
        "y = [233, 91, 111, 152, 120, 67, 310, 94, 183, 66, 173, 72, 49, 64, 48]\n",
        "ypred = [0.0] * 15\n",
        " \n",
        "n = len(x)\n",
        "alpha = 0.01 #learning rate\n",
        "theta0 = 75.1\n",
        "theta1 = -.001\n",
        "num_iterations = 1\n",
        " \n",
        "epochs = 0\n",
        "while epochs < num_iterations:\n",
        "  for i in range(n):\n",
        "    ypred[i] = theta0 + theta1 * x[i] #def of h(x)\n",
        "  if epochs == 0 or epochs == (num_iterations - 1):\n",
        "    plt.scatter(x, y, color='black')\n",
        "    plt.plot(x, ypred, color='blue', linewidth = 3)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.show()\n",
        "  error = 0\n",
        "  total_diff0 = 0\n",
        "  total_diff1 = 0\n",
        "  for i in range(n):\n",
        "    diff = y[i] - ypred[i]\n",
        "    total_diff0 += diff\n",
        "    total_diff1 = diff * x[i]\n",
        "    error += diff**2\n",
        "  error /= 2 * len(y)\n",
        " \n",
        "  theta0 += alpha * total_diff0\n",
        "  theta1 += alpha * total_diff1\n",
        "  print('epoch: ', epochs, 'error: ', error, 'theta0: ', theta0, 'theta1: ', theta1)\n",
        "  print('diff0: ', total_diff0)\n",
        "  print('diff1: ', total_diff1)\n",
        "  epochs +=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:  0 error:  3762.0882962186774 theta0:  82.1649907 theta1:  0.18870489999999993\n",
            "diff0:  706.4990700000003\n",
            "diff1:  18.97048999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-1A67jB5_pk"
      },
      "source": [
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(15 points) Explain what value is generated by the equation $h(x) = \\frac{1}{1+e^{-(\\Theta_0 + \\Theta_1 x)}}$ in logistic regression. What steps are taken to convert the linear regression algorithm into the classification-based logistic regression algorithm?\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "The equation h(x) calculates the probability that x will be in the positive class.\n",
        "\n",
        "In order to convert the linear regression algorithm into the classification-based logistic regression algorithm, we must first modify the decision boundary to smooth out when it reaches y=0 and y=1. This is essentially making the decision boundary into a sigmoid function. We must also update the linear regression cost function so that is can handle gradient descent. The logistic regression algoithm's cost function is $Cost(h(x),y) = -ylog(h(x)-((-y)log(1-h(x))$. \n",
        "\n",
        "---\n",
        "\n",
        "#3.\n",
        "\n",
        "(12 points) Regularization is introduced in class as a numeric term that can be incorporated into a loss function. This is straightforward for algorithms such as neural networks that seek to optimize a numeric loss function. Here, explain possible ways regularization can be used to fine tune other types of algorithms, specifically decision trees and naive Bayes classifiers.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Regularization is applied to decision trees by pruning the learned tree. Too much pruning can result in underfitting the data and too little pruning can result in overfitting the data. Naive Bayes will include all of the features when calculating the most likely. Then through regularixzation, you can prune the features that are irrelevant or redundant.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(80 points) The goal of this program is to give you detailed experience with naive Bayes classifiers as well as with text mining methods and libraries. In this program, you are tasked with writing a naive Bayes classifier to classifier phone messages as spam versus not spam (ham). To test your program, use the labeled data available from the UCI machine learning repository at https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. The data is also available at https://drive.google.com/file/d/1nn2baOauApGbxrOeTb4l-30Qz1prPIS3/view?usp=sharing.\n",
        "\n",
        "You will need to read in each message and convert it to a set of features. Use the sklearn libraries to\n",
        "\n",
        "- remove punctuation\n",
        "- convert to lower case\n",
        "- create a bag of words vector that stores term frequency\n",
        "- filter out words from the stop list (stop_words)\n",
        "- include 1-grams and 2-grams\n",
        "- normalize frequences based on document length (tfidf)\n",
        "\n",
        "Report performance of your classifier on 3-fold cross validation in terms of accuracy and macro f1 score.\n",
        "\n",
        "Additionally, notice that the class distribution is imbalanced. There are 4,827 legitimate messages and 747 spam messages. Experiment with three alternative methods for addressing this issue and report impact of each method on performance.\n",
        "\n",
        "- Undersample the majority class so they are balanced.\n",
        "- Oversample the minority class so they are balanced.\n",
        "- Weight the data points based on class imbalance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkbdzMCoMcIG",
        "outputId": "bcc815b5-a7a0-4a7e-ae57-2d00bd574180"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction import stop_words\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import string\n",
        "import copy\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "def read_data():\n",
        "  infile = open('/content/gdrive/My Drive/CptS437/SMSSpamCollection')\n",
        "  data = []\n",
        "  y = []\n",
        "  for line in infile:\n",
        "    vector = line.strip().lower().split('\\t')\n",
        "    data.append(vector[1])\n",
        "    y.append(vector[0])\n",
        "  return data, y\n",
        "\n",
        "\n",
        "def learn(X, y, weights):\n",
        "  # Train our naive Bayes classifier on our data\n",
        "  clf = MultinomialNB().fit(X, y, sample_weight=weights)\n",
        "  scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
        "  print('Accuracy over 3 folds:', np.mean(scores))\n",
        "  scores = cross_val_score(clf, X, y, cv=3, scoring='f1_macro')\n",
        "  print('Macro f1 score over 3 folds:', np.mean(scores))\n",
        "\n",
        "\n",
        "def main():\n",
        "  data, y = read_data()\n",
        "  count_vect = CountVectorizer()\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  count_vect.set_params(tokenizer=tokenizer.tokenize)\n",
        "\n",
        "  # 1-grams and 2-grams\n",
        "  count_vect.set_params(ngram_range=(1,2))\n",
        "\n",
        "  X_counts = count_vect.fit_transform(data)\n",
        "\n",
        "  # Normalize counts and weight common words low\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "\n",
        "  # Learn with undersampling the majority class\n",
        "  print('Undersampling the Majority Class:')\n",
        "  undersample = RandomUnderSampler(sampling_strategy='majority')\n",
        "  X_tfidfu, yu = undersample.fit_resample(X_tfidf, y)\n",
        "  clfu = MultinomialNB().fit(X_tfidfu, yu)\n",
        "  scoresAcc = cross_val_score(clfu, X_tfidfu, yu, cv=3, scoring='accuracy')\n",
        "  print('Number of samples: ', len(yu))\n",
        "  print('3-fold accuracy: ', np.mean(scoresAcc))\n",
        "  scoresF1 = cross_val_score(clfu, X_tfidfu, yu, cv=3, scoring='f1_macro')\n",
        "  print('3-fold macro f1: ', np.mean(scoresF1), '\\n')\n",
        "\n",
        " # Learn with oversampling the minority class \n",
        "  print('Oversampling the Minority Class:')\n",
        "  oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "  X_tfidfo, yo = oversample.fit_resample(X_tfidf, y)\n",
        "  clfo = MultinomialNB().fit(X_tfidfo, yo)\n",
        "  scoresAcc = cross_val_score(clfo, X_tfidfo, yo, cv=3, scoring='accuracy')\n",
        "  print('Number of samples: ', len(yo))\n",
        "  print('3-fold accuracy: ', np.mean(scoresAcc))\n",
        "  scoresF1 = cross_val_score(clfo, X_tfidfo, yo, cv=3, scoring='f1_macro')\n",
        "  print('3-fold macro f1: ', np.mean(scoresF1), '\\n')\n",
        "\n",
        "  # Learn with equal weights and no sampling\n",
        "  print('Unweighted:')\n",
        "  weights = np.full(len(y), 1.0, dtype=float)\n",
        "  learn(X_tfidf, y, weights)\n",
        "\n",
        "  # Learn with weights inversely proportional to class size\n",
        "  weights = compute_sample_weight(\"balanced\", y)\n",
        "  print('Weighted:')\n",
        "  print('Weights', weights)\n",
        "  learn(X_tfidf, y, weights)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Undersampling the Majority Class:\n",
            "Number of samples:  1494\n",
            "3-fold accuracy:  0.9618473895582329\n",
            "3-fold macro f1:  0.9618394655377521 \n",
            "\n",
            "Oversampling the Minority Class:\n",
            "Number of samples:  9654\n",
            "3-fold accuracy:  0.9925419515226849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3-fold macro f1:  0.9925417490591067 \n",
            "\n",
            "Unweighted:\n",
            "Accuracy over 3 folds: 0.923394330821672\n",
            "Macro f1 score over 3 folds: 0.7786465483571515\n",
            "Weighted:\n",
            "Weights [0.57737725 0.57737725 3.73092369 ... 0.57737725 0.57737725 0.57737725]\n",
            "Accuracy over 3 folds: 0.923394330821672\n",
            "Macro f1 score over 3 folds: 0.7786465483571515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4khRUNYWMlbu"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "#5.\n",
        "\n",
        "(10 points, part of semester project grade)\n",
        "\n",
        "Provide a proposal of your semester project. The proposal is typically 1-2 paragraphs long and should include the goal of your project, a description of the data you will use and any necessary data cleaning / preparation steps you will need to perform, the methods you will create or apply, and how you will evaluate the results. If you are working on a multi-person project team, clearly state the role of each person on the team in contributing to the project goal.\n",
        "\n",
        "\n",
        "\n",
        "Morgan Baccus and Brenden Nelson\n",
        "\n",
        "For our semester project we are going to modify the decision tree algorithm to predict if an accident will be light or fatal. Our data shows if an accident is light or fatal based on a number of features such as date, time, longitude, latitude, and many more. This data is provided on Kaggle as part of the Road Accidents in U.K. data collection. As many values are missing for some features, we will not be analyzing them. Looking at the data, it appears that there is an imbalance between the light and fatal classifications. We may need to address the imbalance by removing some of the larger class or by assigning weights. Additionally, we will be using cross-validation when analyzing the data so that we can look at different segments as training and testing each time. We will be looking at accuracy and a confusion matrix to evaluate the results of our algorithm. This will provide several calculations for us to compare how well our algorithm performed.\n",
        "\n",
        "Morgan will clean and prepare the data. This will include eliminating the features that have too many missing values, fixing the class imbalance if there is one, and whatever else needs to be done for our algorithm to be able to process the data. We will work on building the decision tree algorithm together as this is the main part of the project. Brenden will then go through and prune the tree. Morgan will implement the necessary functions to evaluate our tree. Brenden will then continue to prune the tree until the best results are achieved.\n"
      ]
    }
  ]
}