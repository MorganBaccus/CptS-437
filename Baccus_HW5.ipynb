{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baccus.HW5",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganBaccus/CptS-437/blob/main/Baccus_HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF1vAj5wklz1"
      },
      "source": [
        "# **Homework Assignment #5**\n",
        "\n",
        "Assigned: March 24, 2021\n",
        "\n",
        "Due: April 12, 2021\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "This assignment consists of questions that require a short answer and one Python programming task. You can enter your answers and your code directly in a Colaboratory notebook and upload the **shareable** link for the your notebook as your homework submission.\n",
        "\n",
        "---\n",
        "\n",
        "#1.\n",
        "\n",
        " (12 points) In most real world scenarios, data contain outliers. When using a support vector machine, outliers can be dealt with using a soft margin, specified in a slightly different optimization problem shown in Equation 7.38 in the text and called a soft-margin SVM.\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i = 0$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $0 < \\zeta_i \\leq 1$? Is this data point classified correctly?\n",
        "\n",
        "Intuitively, where does a data point lie relative to the margin when $\\zeta_i > 1$? Is this data point classified correctly?\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "All data points that lie relative to the margin when $\\zeta_i = 0$ will be classified as a positive example and that is the correct classification.\n",
        "\n",
        "All data points that lie relative to the margin when $0 < \\zeta_i \\leq 1$? will be classified as a positive example and that is the correct classification.\n",
        "\n",
        "All data points that lie relative to the margin when $\\zeta_i > 1$? will be classified as a negative example and that is the incorrect classification.\n",
        "\n",
        "---\n",
        "\n",
        "#2.\n",
        "\n",
        "(12 points) Suppose the two-layer neural network shown below processes the input (0, 1, 1, 0). If the actual output should be 0.2, show step-by-step how the vector of weights *v* will be updated using backpropagation and $\\eta = 0.2$.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1mLkFgXA0drWp6nYL50n0BZv2Z13EA9CN)\n",
        "\n",
        "Answer:\n",
        "![](https://drive.google.com/uc?id=1YzEmQGrTPVcakPRG1X1Db2srdpEagQ3D)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#3. \n",
        "\n",
        "(8 points) Under which of these conditions does an ensemble classifier perform best? There can be more than one right answer, explain all of your responses.\n",
        "\n",
        "- Low prediction correlation between base classifiers.\n",
        "- High prediction correlation between base classifiers.\n",
        "- Base classifiers have low variance.\n",
        "- Base classifiers have high bias.\n",
        "- Base classifiers have high variance.\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "An ensemble classifier performs best when there is a low correlation between base classifiers as it increases the error-correcting capability of the ensemble. Usually weak learners are used in ensemble methods as they have low bias and low variance which prevents them from overfitting the training data.\n",
        "\n",
        "---\n",
        "\n",
        "#4.\n",
        "\n",
        "(80 points) The goal of this problem is for you to implement backpropagation from scratch. You can make use of python libraries for handling the data and computation, but implement the actual activation and weight change calculations yourself.\n",
        "\n",
        "Test your neural network using the MNIST dataset. Information on loading and storing this handwritten-digit dataset can be found at https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html. Only consider digit classes '0' (which you can map onto value -1) and '1' (which you can map onto value 1). Train the network on a randomly-selected 2/3 of the data points and test on the remaining 1/3. You can report mean squared error or accuracy for the test data for a minimum of 10 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfwejW_M62n-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b90aae8-6602-4af1-99ba-d6751afc38c0"
      },
      "source": [
        "from math import exp\n",
        "from random import random\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "  network = list()  # initialize weights to random number in [0..1]\n",
        "  hidden_layer = [{'weights':[random() for i in range(n_inputs+1)]} for i in range(n_hidden)]\n",
        "  network.append(hidden_layer)\n",
        "  output_layer = [{'weights':[random() for i in range(n_hidden+1)]} for i in range(n_outputs)]\n",
        "  network.append(output_layer)\n",
        "  return network\n",
        "\n",
        "\n",
        "def activate(weights, inputs):\n",
        "  activation = weights[-1]   # bias\n",
        "  for i in range(len(weights)-1):\n",
        "    activation += weights[i] * inputs[i]\n",
        "  return activation\n",
        "\n",
        "\n",
        "def transfer(activation): # sigmoid function\n",
        "  return 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "\n",
        "def forward_propagate(network, X, y):\n",
        "  inputs = X\n",
        "  for layer in network:\n",
        "    new_inputs = []\n",
        "    for node in layer:\n",
        "      activation = activate(node['weights'], X)\n",
        "      node['output'] = transfer(activation)\n",
        "      new_inputs.append(node['output']) # output of one node input to another\n",
        "    inputs = new_inputs\n",
        "  return inputs   # return output from last layer\n",
        "\n",
        "  \n",
        "def transfer_derivative(output): # derivative of sigmoid function\n",
        "  return output * (1.0 - output)\n",
        "\n",
        "\n",
        "def backward_propagate_error(network, expected):\n",
        "  for i in reversed(range(len(network))): # from output back to input layers\n",
        "    layer = network[i]\n",
        "    errors = list()\n",
        "    if i != len(network)-1:  # not the output layer\n",
        "      for j in range(len(layer)):\n",
        "        error = 0.0\n",
        "        for node in network[i+1]:\n",
        "          error += (node['weights'][j] * node['delta'])\n",
        "        errors.append(error)\n",
        "    else:   # output layer\n",
        "      for j in range(len(layer)):\n",
        "        node = layer[j]\n",
        "        errors.append(expected[j] - node['output'])\n",
        "    for j in range(len(layer)):\n",
        "      node = layer[j]\n",
        "      node['delta'] = errors[j] * transfer_derivative(node['output'])\n",
        "\n",
        "\n",
        "def update_weights(network, x, y, eta):\n",
        "  for i in range(len(network)):\n",
        "    inputs = x\n",
        "    if i != 0:\n",
        "      inputs = [node['output'] for node in network[i-1]]\n",
        "    for node in network[i]:\n",
        "      for j in range(len(inputs)):\n",
        "        node['weights'][j] += eta * node['delta'] * inputs[j]\n",
        "      node['weights'][-1] += eta * node['delta']\n",
        "\n",
        "\n",
        "def train_network(network, X, y, eta, num_epochs, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  for epoch in range(num_epochs):\n",
        "    sum_error = 0\n",
        "    # There are two output nodes. The one corresponding to the correct label\n",
        "    # should output 1, the other should output 0.\n",
        "    for i in range(len(y)):\n",
        "      outputs = forward_propagate(network, X[i], y[i])\n",
        "      if y[i] == 0:\n",
        "        expected[0] = 1\n",
        "        expected[1] = 0\n",
        "      else:\n",
        "        expected[0] = 0\n",
        "        expected[1] = 1\n",
        "      sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "      backward_propagate_error(network, expected)\n",
        "      update_weights(network, X[i], y[i], eta)    \n",
        "    print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, eta, sum_error))\n",
        "\n",
        "\n",
        "def test_network(network, X, y, num_outputs):\n",
        "  expected = np.full((2), 0)\n",
        "  sum_error = 0\n",
        "  # There are two output nodes. The one corresponding to the correct label\n",
        "  # should output 1, the other should output 0.\n",
        "  for i in range(len(y)):\n",
        "    outputs = forward_propagate(network, X[i], y[i])\n",
        "    if y[i] == 0:\n",
        "      expected[0] = 1\n",
        "      expected[1] = 0\n",
        "    else:\n",
        "      expected[0] = 0\n",
        "      expected[1] = 1\n",
        "    sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
        "  print('mse of test data is', sum_error / float(len(y)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Load data from https://www.openml.org/d/554\n",
        "  features, targets = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "  X = []\n",
        "  y = []\n",
        "  for i in range(len(targets)):\n",
        "    if targets[i] == '1' or targets[i] == '0':\n",
        "      X.append(features[i])\n",
        "      if targets[i] == '0':\n",
        "        y.append(0)\n",
        "      else:\n",
        "        y.append(1)\n",
        "  n_inputs = len(X[0])\n",
        "  n_outputs = 2  # possible class values are '0' and '1'\n",
        "  # Create a network with 1 hidden layer containing 2 nodes\n",
        "  network = initialize_network(n_inputs, 2, n_outputs)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67, test_size=0.33)\n",
        "  # train network for 10 epochs using learning rate of 0.1 \n",
        "  train_network(network, X_train, y_train, 0.1, 10, n_outputs)\n",
        "  for layer in network:\n",
        "    print('layer \\n', layer)\n",
        "  test_network(network, X_test, y_test, n_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">epoch=0, lrate=0.100, error=4938.336\n",
            ">epoch=1, lrate=0.100, error=4932.779\n",
            ">epoch=2, lrate=0.100, error=4932.779\n",
            ">epoch=3, lrate=0.100, error=4932.779\n",
            ">epoch=4, lrate=0.100, error=4932.779\n",
            ">epoch=5, lrate=0.100, error=4932.779\n",
            ">epoch=6, lrate=0.100, error=4932.779\n",
            ">epoch=7, lrate=0.100, error=4932.779\n",
            ">epoch=8, lrate=0.100, error=4932.779\n",
            ">epoch=9, lrate=0.100, error=4932.779\n",
            "layer \n",
            " [{'weights': [0.15315925438105815, 0.22121148977270944, 0.9230053149417814, 0.8841761509349702, 0.22544996190714628, 0.3213160634263741, 0.5709395849605104, 0.22949009122542618, 0.9764813265925268, 0.3623313423181016, 0.4980515574181761, 0.2073275160372403, 0.4546156471676992, 0.03807880062892677, 0.24086225454323074, 0.9750374957815325, 0.03824921695479422, 0.5585564502795652, 0.8205124093091326, 0.4294469364546788, 0.44755250034443195, 0.28754041343228254, 0.8168983185738857, 0.36183146261044674, 0.9873338755099496, 0.2038514250388539, 0.569750572558605, 0.779634269779047, 0.0364197319301377, 0.9887183158072066, 0.18757545593340308, 0.20230853027879214, 0.7803024053937698, 0.9901679219217276, 0.11185410348071834, 0.8365603082648326, 0.7396458408431756, 0.2763183626682202, 0.46308197238521354, 0.5208563002262079, 0.8138564262768309, 0.6573650847961859, 0.9640860043617331, 0.8898034813705952, 0.05533244665848003, 0.21747861825110915, 0.5135731527496868, 0.975802855766698, 0.023641939764437847, 0.8517876054266872, 0.13482375581163175, 0.8159007786815567, 0.7296965152619571, 0.3912055183032678, 0.28796725790310096, 0.4625899998556061, 0.5757654141376102, 0.8544598338755097, 0.5838637384530498, 0.8424642280802603, 0.05566539809071003, 0.14084718755168724, 0.3743500607197694, 0.770854050195434, 0.17517601876261923, 0.9552491834461031, 0.7147579142952906, 0.9036038102753111, 0.07697678732941671, 0.2231352787138402, 0.31078057060790276, 0.7359666701228592, 0.7236749793981537, 0.4498771904324599, 0.8237867842005251, 0.9687236697055496, 0.6592241566058633, 0.08107439514526349, 0.40407429789918636, 0.0634334590958352, 0.5265400467680198, 0.3803771469532883, 0.0913332329660912, 0.9818556931046817, 0.9953440938320003, 0.5112926361681568, 0.0803618910218461, 0.527843312078959, 0.11423865565204394, 0.5186573034032003, 0.055883221469241295, 0.6546083208251143, 0.9220809949663402, 0.6303152733458108, 0.6366652903987258, 0.47401800105601233, 0.12073078453362507, 0.1129884771805163, 0.6917436378811732, 0.8329109671699837, 0.0037365466684192716, 0.02543246394666654, 0.05367179814131895, 0.8329621600665519, 0.06776256283400728, 0.9955178357873186, 0.8649622861010962, 0.0021568996829803844, 0.9771943275527093, 0.29839019362371866, 0.7545915606484218, 0.9507082598116667, 0.2820793453727941, 0.22694928782610713, 0.3661399242854966, 0.9210372306819908, 0.829658580826781, 0.1771515986954486, 0.5452995068459775, 0.36309089156705576, 0.6452011834706377, 0.8552301515324277, 0.9781689931329014, 0.05994005901028243, 0.3209708054803524, 0.7853970236607255, 0.7325675766059538, 0.6623410083861909, 0.2929176597174372, 0.914942350997227, 0.6744245223458727, 0.766009412672505, 0.3562608017686827, 0.6830327083662667, 0.1731454859127871, 0.7163086052441634, 0.03921987699285068, 0.8773599987272688, 0.28526390494453446, 0.10842999312405743, 0.35554331697324826, 0.8328087176252721, 0.957614876394182, 0.7803941623599208, 0.09849597418189882, 0.5358970801119961, 0.8010341579246155, 0.6701359147377701, 0.2047514050780339, 0.6812258582609602, 0.8865772142510066, 0.21465907414407737, 0.5793545588772167, 0.20136239047556936, 0.48246093012645197, 0.16798386004508137, 0.5828927431655222, 0.8446254803347584, 0.28527180726162005, 0.27171705672604807, 0.9559304944248099, 0.581844794352042, 0.7101065920863038, 0.6958413772156653, 0.9180421511553744, 0.8701104897653107, 0.5492363162469752, 0.9498475740428044, 0.9076252412404814, 0.6202079549366899, 0.1144472338635486, 0.9410225939551126, 0.14146560016990495, 0.9381672823444066, 0.8593536305839268, 0.9740311298998982, 0.6662219441379541, 0.34364720925782155, 0.982281129363908, 0.6583096398667602, 0.8216334988577657, 0.7848094867536037, 0.8151589051814649, 0.4886544765619303, 0.4781820002362819, 0.09952015416515347, 0.647196628295389, 0.006315063274752397, 0.44930982948432896, 0.675991615242191, 0.9552593002436996, 0.7913073606214293, 0.39963550222132427, 0.3989722572932779, 0.21932352394784238, 0.6581907650735708, 0.042043177598267034, 0.9331517616145918, 0.45861259176544866, 0.372417090942454, 0.9619611664792422, 0.6591391390662497, 0.9720395161520529, 0.44691689052196015, 0.5380435698987919, 0.40191750734483866, 0.9132601479110543, 0.7395343945474648, 0.9948811329947664, 0.025359176485666546, 0.8152904898948269, 0.04938846179843526, 0.7263337675344971, 0.15606690510801857, 0.39962662804556504, 0.05253974410135265, 0.38426447253230966, 0.026896211345536214, 0.4379670322537007, 0.4627316697859447, 0.7426840106416551, 0.9210499126804444, 0.689340311839141, 0.14892119052902753, 0.047685665650633036, 0.31628210346340446, 0.3199159107621341, 0.8601545300671987, 0.23898142752321783, 0.4091705438148827, 0.5467386348618835, 0.686632854457099, 0.6138154406958211, 0.21105317976630134, 0.5450492149754815, 0.6880263895731769, 0.8642655865115364, 0.211637263029272, 0.29540190344841877, 0.4531697778974162, 0.6711859080018946, 0.19040099535691068, 0.7893989084912599, 0.116070839270776, 0.12646852649395024, 0.09257880541743879, 0.15576862848064044, 0.7353032898515732, 0.779532476583836, 0.4485104574926787, 0.12113339347567886, 0.4605712451612757, 0.8642619747042527, 0.8924818770985931, 0.09140184958831199, 0.029645339840349005, 0.08845584144272656, 0.43611806258333685, 0.6041436730996881, 0.7666436360072861, 0.948259655768564, 0.5466999704439925, 0.9747696836643548, 0.8772832152685579, 0.9558239681205033, 0.9190827631968098, 0.9749829073385167, 0.7109397736824616, 0.08783768595422703, 0.03561758868676512, 0.4565828908627504, 0.8349608430423013, 0.5564653005480833, 0.8354795826999872, 0.3578103774625888, 0.45936002922761454, 0.02866335200356307, 0.5394739725002055, 0.22715533876665284, 0.36416042006048055, 0.5560591208277696, 0.5777162971752438, 0.5229762063722136, 0.27692081638558386, 0.17096630294050463, 0.8868338718106763, 0.464587653426335, 0.2524098513024212, 0.2261796815577436, 0.044512762327615585, 0.769164582475308, 0.2511655424238983, 0.5142558700429376, 0.19111261598686224, 0.3486286528380428, 0.674536230659341, 0.6235865016339296, 0.2080384782158602, 0.17904101628153257, 0.6342627046987175, 0.3965645082144049, 0.20278720797210947, 0.7181138528431088, 0.65367758069851, 0.4225046842472133, 0.84055216097469, 0.17125676975445225, 0.30103545684402566, 0.5175712054949319, 0.1543620563960616, 0.5593661657184419, 0.44607528302650146, 0.010736225613985129, 0.37293310508587596, 0.9501639608557719, 0.24349536166688623, 0.8500548178424552, 0.121137786314224, 0.9596109827912445, 0.8256709653803876, 0.4079043960771058, 0.48603875194774215, 0.9896674696518571, 0.639833224560279, 0.24094568797015448, 0.4796959814630032, 0.23986873361319705, 0.6734069222751762, 0.40106119100669535, 0.5668887769364754, 0.9622815677580671, 0.01462239723016523, 0.2003850612893313, 0.6183125426626905, 0.8670015712878295, 0.7606597905720841, 0.7529743335656509, 0.1240052195533492, 0.8276091454733163, 0.32820209922963917, 0.7282901566687664, 0.03382706621586529, 0.9973505279011861, 0.2149672764017292, 0.6700384240933849, 0.2854869004428793, 0.5033578456327246, 0.4372039846022847, 0.10292606486025113, 0.4707559522725785, 0.7208295147956835, 0.8728745618680357, 0.7622917705124022, 0.6685613023595129, 0.958422058658855, 0.902972607712912, 0.4032098553946436, 0.5543668226785529, 0.07536542313235584, 0.8847114671725823, 0.7485440612737722, 0.3496123066610013, 0.32463142926808874, 0.12878049470076147, 0.7026960965737106, 0.4207361256961214, 0.09339843429722461, 0.3125725064324333, 0.8054255074848296, 0.28484812802544, 0.4876982828044165, 0.1952798661569951, 0.4396163353534779, 0.5377870784471652, 0.3137682262765087, 0.6046613079035662, 0.2750242985681298, 0.058962801959092315, 0.6127303909578463, 0.31429095915736527, 0.557053546352958, 0.1419346053050461, 0.9471675699244493, 0.884965587354977, 0.631085815641605, 0.6998404639147884, 0.5503300563692091, 0.48702818491708033, 0.46847859743873677, 0.11414116975128918, 0.8356348531667216, 0.7347339615487286, 0.13841683145270156, 0.8739147709828793, 0.7629414802307987, 0.9478696871122875, 0.7056619054420351, 0.8938278414160873, 0.29943586849616155, 0.8337045512272256, 0.9440748391492024, 0.6605528214601429, 0.8430743315800657, 0.2596060811211618, 0.42431243704730737, 0.6656147176965062, 0.5638458269556023, 0.5083320870381869, 0.23285813420757473, 0.3454833946780661, 0.28457399027640906, 0.5400089995934055, 0.0685452120925879, 0.9444713611903978, 0.8502855600883021, 0.9077864260865807, 0.3310049168121245, 0.7387585016426339, 0.3708883961161282, 0.0599636856413861, 0.5063255641323887, 0.844153806877181, 0.5107256480408815, 0.2538699602638811, 0.7142883536811029, 0.5111044811290801, 0.3657363299944144, 0.08762256034838756, 0.0784045097019368, 0.6485631264487006, 0.9529933486644867, 0.5212109337573001, 0.9790123535775581, 0.9872822369788063, 0.21804365852336682, 0.7878835247760617, 0.5103169334570875, 0.1849450041169861, 0.27512233817274745, 0.44999287233550556, 0.540365296479738, 0.19472769912052101, 0.4623953627766325, 0.050245332925108466, 0.5103270364465882, 0.41396891771650124, 0.25506083938901025, 0.7899083425663613, 0.16001542318874729, 0.49752277948807744, 0.19653299241003253, 0.928701230486677, 0.6700408407353412, 0.18532151472139002, 0.5385834836955454, 0.0967540071815367, 0.7632000079587686, 0.30208209748298, 0.04746465521059917, 0.24999790475440964, 0.3305757058412713, 0.8090559375690826, 0.014819934266329216, 0.3720101146745607, 0.23261382763352623, 0.19491651822765488, 0.39622731842533077, 0.3741203876522986, 0.001190013307951987, 0.5311216589018478, 0.3204434100946968, 0.44405599777786264, 0.08766490012938699, 0.337208924098086, 0.6471493974336112, 0.4499269515012906, 0.6967816965721794, 0.42948324220482914, 0.49280585120135445, 0.36173976284453724, 0.10304900046496834, 0.3497992409002404, 0.4808207494640968, 0.041197287390355, 0.2881170907215558, 0.715382061658547, 0.8315251547968019, 0.1020010507719783, 0.51951588059759, 0.36347459259201265, 0.08347046138569059, 0.13294886189404354, 0.31757404141651147, 0.9278290634420532, 0.5420833145534396, 0.3461894139122246, 0.8687338523568038, 0.693733772884503, 0.4250297169505496, 0.9605909229573689, 0.299331996847384, 0.02205541000817246, 0.9765806798350314, 0.42015284947588016, 0.4235125000561828, 0.37313899953807617, 0.9008173267837517, 0.7647019996312523, 0.1684087105614399, 0.45203382217200605, 0.42030193229884616, 0.39922450542235444, 0.004226811925002627, 0.8999280106373618, 0.8655558193437202, 0.19907105507756484, 0.7091473213600726, 0.4235868030113129, 0.6677936873030885, 0.8106647441807587, 0.02242207501212401, 0.8698031245386401, 0.9578956128673786, 0.8595382540738035, 0.4442672881460259, 0.9864920440499632, 0.2196927622741165, 0.7812940729118306, 0.454635889880462, 0.8439348889720889, 0.40497217120497186, 0.4532115255247411, 0.8328500860363096, 0.6733159454837065, 0.9634094671761354, 0.8057607577803874, 0.655251007852718, 0.9680940904565344, 0.8988028208349862, 0.4158333563642749, 0.7518526545119396, 0.8345300282555749, 0.34731912292607614, 0.32074131726772037, 0.7766422755330095, 0.8941076605857946, 0.5480474278873431, 0.5935249469264036, 0.4299951769268141, 0.5122237323069851, 0.6528797779503854, 0.3732723790962904, 0.49039071786109667, 0.6468980383086681, 0.3738864657243448, 0.8253138737444478, 0.7517951287470555, 0.5607515882156994, 0.42625820109308143, 0.7876025932732307, 0.0620499071881494, 0.34346942040801043, 0.2533514346224206, 0.3850051870396115, 0.7661315392362591, 0.4884518932876363, 0.7053156646846974, 0.004210536656549024, 0.9258487032067023, 0.438812449695272, 0.5880434213022638, 0.9617888792094637, 0.5210880295075028, 0.4040836178091535, 0.15713348242749914, 0.2608913904402198, 0.7316534829141804, 0.7677562645612115, 0.7692277980551689, 0.1499296952950686, 0.3547464041323709, 0.3637537288645317, 0.4101961378184926, 0.6507135150837229, 0.7969239792697167, 0.29556423589909253, 0.5839706792597146, 0.9082663099239097, 0.8283373814117978, 0.9166254656050928, 0.31351634234627224, 0.4552487894085627, 0.8953059872834533, 0.32063809300166923, 0.9006851798917493, 0.5045860006186986, 0.9161723686126211, 0.6563258372219369, 0.7235728904671891, 0.6841356148082947, 0.16797863399804513, 0.40739188190426956, 0.10036826061054827, 0.8349130160007542, 0.0931953858205421, 0.970822835484293, 0.6136704083695201, 0.0838791519692843, 0.7667872101879473, 0.310880005134489, 0.14302592805427095, 0.034512249149347585, 0.01267722931476778, 0.880472933987579, 0.37096421930723755, 0.7590671494863118, 0.31141283025352295, 0.8476991946671812, 0.7116522753300437, 0.976055513343695, 0.35380445003150607, 0.6650195366004243, 0.6959922010129217, 0.7427447698414245, 0.5909108272833351, 0.5370441708307457, 0.30666089323506773, 0.7529017371806279, 0.5955219045798034, 0.6738533547589343, 0.6926163140230566, 0.8600189921053549, 0.438265292770999, 0.6845312379103374, 0.41815395251553833, 0.35125228025312694, 0.6382725028028847, 0.8503177900485557, 0.8359157320642635, 0.7104628801291161, 0.11756879959278077, 0.36476104085329486, 0.36171811477738913, 0.18242732997603184, 0.005056530401538306, 0.5472436960926892, 0.6276711516244714, 0.23180134631444682, 0.48155915845863795, 0.9889252415499895, 0.6602995186738592, 0.5363452011977362, 0.19841667298035148, 0.2780046569370468, 0.5513269585364292, 0.45339812890477627, 0.7807024465552796, 0.710789869050655, 0.790312511522832, 0.4149967988474409, 3.5368402984126135e-08, 0.05303630321758279, 0.4310098300783546, 0.13771308845087138, 0.8392973627727309, 0.4544322269440081, 0.43649630901130654, 0.06565939897018869, 0.7754396162320569, 0.7389037028489588, 0.5228971595364462, 0.7352459312119476, 0.5453987213808045, 0.1884462997039129, 0.42333819732828126, 0.6602587831363788, 0.38324576426417156, 0.8564232907871597, 0.8920676209145676, 0.6401185470986821, 0.3445642947185448, 0.15880119150898087, 0.5643175088564389, 0.7905083363506915, 0.15320134844813404, 0.10800035544683628, 0.800721742618406, 0.7844113450362952, 0.9588781019365238, 0.1711218966014454, 0.12220203884299141, 0.06922047627354133, 0.5601075336415975, 0.09345918746222581, 0.8060257204724989, 0.7591885940316261, 0.3582110204633536, 0.060382423367208116, 0.45588193783975184, 0.2104505868349329, 0.7162800247190051, 0.4515113585064554, 0.4901844146321013, 0.32149407262799956, 0.25483427697806926, 0.9170498255606118, 0.7538286970259745, 0.025879040368083084, 0.00044920700375106204, 0.14361000622501607, 0.9288204001078569, 0.37024191347975166, 0.4683310776697447, 0.9398425469304182, 0.05324413546605178, 0.35594939933427383, 0.959191482521139, 0.37331907403631626, 0.45011053238347476, 0.6756319290654467, 0.19912621262525165, 0.8759984620047103, 0.3743535288305585, 0.34733713655091647, 0.6841311173428386, 0.3668585992373681, 0.21588032580109284, 0.523690208821757, 0.14532662137832053, 0.6268193342454056, 0.4082319257884246, 0.2680818186226933, 0.13712442745370534, 0.6454281264128005, 0.19910123591100215, 0.3825660551938641, 0.6106889661180809, 0.5919055633391445, 0.6250335066465469, 0.32205905093274534, 0.5615290005877991, 0.3247734147639473, 0.6438373165341097, 0.1458530732185812, 0.11992102220431444, 0.41850117615775395, 0.666129101280831, 0.04012169008540545, 0.47956771532603637, 0.32226939417167255, 0.3887080585143069, 0.5897499422197343, 0.6586541433671328, 0.6926560709626648, 0.5013579720989435, 0.11708036213964845, 0.3725155214273427, 0.6148397760887835, 0.8207364078990199, 0.5066449727143976, 0.8216255793231232, 0.6072322356460921, 0.3430845467192659, 0.22515902687064127, 0.6012832213746515, 0.9528453477208404, 0.12157425961242574, 0.8914714472523319, 0.06657941884916196, 0.9870589315835503, 0.5325812793126854, 0.08516840065386067, 0.6080873676720979, 0.23778236181671653, 0.20865348091959868, 0.909572536825776, 0.42339619460130673, 0.39784504090863715, 0.6876541790236526, 0.3693372105385092, 0.5577285787013104, 0.6925830453488503, 0.23279903770258026, 0.18728627075205107, 0.912308320223229], 'output': 1.0, 'delta': -0.0}, {'weights': [0.8537340713298385, 0.7164059670608798, 0.0142852325676871, 0.5293901133854972, 0.45496825062928103, 0.9603731554911277, 0.8613802336206617, 0.36298034666930723, 0.40762583329015634, 0.390729171335386, 0.6225425469583779, 0.4070280372300823, 0.2993018272446296, 0.04240256562686273, 0.4628188373971813, 0.3727881202026667, 0.44959641869956524, 0.13311374712626656, 0.08238838500371481, 0.16774486085490592, 0.7957661264007101, 0.8377446120766935, 0.053786325792893774, 0.8685923705995535, 0.531637054049852, 0.3106868023907686, 0.6765990218636785, 0.9399656359016485, 0.620921438368072, 0.9637549049415286, 0.7359665316245745, 0.07143114922230387, 0.08090169010737902, 0.010131469865676035, 0.279555810897829, 0.5167344315795516, 0.12004898848773782, 0.9067252456996688, 0.13400264729117495, 0.44934263549831743, 0.367321877559847, 0.5127772949149997, 0.06618466981002857, 0.3636643627748496, 0.20102908790760876, 0.35921405403953366, 0.060402696802384415, 0.020852040548343997, 0.7356368383102134, 0.20532516925008792, 0.7757675825189717, 0.005731511985024529, 0.11457320859844744, 0.02394539307195831, 0.47348945643396456, 0.19644782026145624, 0.16263038503894456, 0.32632746732897955, 0.8013087759977711, 0.17553798730013392, 0.056842890075495256, 0.40696341943822556, 0.024630623945568253, 0.5069773051158428, 0.6437128898789054, 0.8818254220106195, 0.8396692632341682, 0.965331502999536, 0.390991590231604, 0.5743964975663084, 0.05788669708412886, 0.37999643610209055, 0.16924556944968483, 0.3519719509043967, 0.44085487269418044, 0.41561820463905286, 0.5078060687338417, 0.706115093295535, 0.9898981836065697, 0.6848557636393386, 0.043466959201174715, 0.6386873392894548, 0.027331792672885435, 0.7539042746565089, 0.9092659733354703, 0.6528103168030741, 0.5685737845660794, 0.8531917886836186, 0.7441127533069823, 0.29656476203469373, 0.5925998159013282, 0.12508710861709926, 0.4413311257042939, 0.4220458622762502, 0.09326772859326271, 0.4742331626646563, 0.023421688264354756, 0.45645673637138684, 0.5000766306317724, 0.7286219072138935, 0.4621734050912646, 0.4229677387837635, 0.37597620276649724, 0.5888272709643392, 0.4347555401315232, 0.16263557523214445, 0.0712155827536699, 0.4461851128392931, 0.17829560614717854, 0.9124115002310168, 0.8126005937228749, 0.5180082112086098, 0.33340666569700295, 0.7818522646081856, 0.9267251990833457, 0.8633619106972962, 0.6273912259439282, 0.11244143030401976, 0.23208258159870154, 0.11940723934357311, 0.5441076793344052, 0.5907640223504722, 0.9371217278968087, 0.8882251692020126, 0.2770646727243492, 0.17183353589546235, 0.5646473649372616, 0.4756772794000136, 0.7666193718739893, 0.38913433020684296, 0.6817441699933574, 0.6777226625897296, 0.6066313974937498, 0.36958623741239216, 0.6856219811668608, 0.7355468616937567, 0.6512886256941824, 0.20232894169146753, 0.012057598051417817, 0.9215654083850315, 0.06749678886662747, 0.9107626902790363, 0.7376435190231752, 0.5638812255892631, 0.3835476030359616, 0.18740653870626356, 0.28614326803619183, 0.15761822609889042, 0.9943598092167483, 0.182154016001286, 0.7181950059617065, 0.857253396806201, 0.8130764304802232, 0.5752326071817172, 0.22016232367799282, 0.9136685826100088, 0.5289239322037037, 0.5903682519764186, 0.23885259793167024, 0.7999028041825175, 0.16692775321039988, 0.9215453087394758, 0.5427652821264927, 0.2481471443104598, 0.2954622022114455, 0.01661468079248618, 0.0631092934606643, 0.8808463463246132, 0.8698207857121355, 0.7001628269822054, 0.8471568544194606, 0.324161241538789, 0.6589265354460084, 0.1383468535521254, 0.16476016501786506, 0.7222091026055273, 0.893960975056361, 0.023968342680432353, 0.5159586819490518, 0.33803946805286444, 0.013181153029355741, 0.773972320070296, 0.3853694897093669, 0.7624027591726281, 0.3083294155473938, 0.9251060381726615, 0.3405225168457804, 0.01823273121031499, 0.42134532386610046, 0.1818543591253754, 0.5435025643624759, 0.4608618062432126, 0.015405137544242331, 0.5884987613223255, 0.7138555445140675, 0.21222154012066274, 0.39891209173289577, 0.1626573718865596, 0.3865409411488544, 0.4436319989778026, 0.5662656672855374, 0.40095838823895436, 0.7715442416055918, 0.7842536325513254, 0.3933643198283643, 0.36482099187774353, 0.8013343204711606, 0.14375240390000754, 0.06913027729616172, 0.21954622565963133, 0.44856480275392874, 0.9289129886992086, 0.09602427972505123, 0.9265440820186372, 0.06961886952658014, 0.5117301052773559, 0.3228333928218142, 0.7387188705613649, 0.3620369666432718, 0.9827522594101965, 0.7181651031242079, 0.7110262245517556, 0.4081729677392971, 0.08886513909880323, 0.08322407390433317, 0.917619974628956, 0.633041729539769, 0.6216858900582479, 0.0617915759114811, 0.2902766896308697, 0.25730758399036513, 0.06426068067419899, 0.5958335055974094, 0.9527934654678166, 0.46051538539632964, 0.5981629734488398, 0.741467987118054, 0.41006072739538235, 0.0037203762314921285, 0.12877634134122173, 0.0492036957620241, 0.23397305099363408, 0.7363061805666414, 0.8356526344781977, 0.973385761284899, 0.8240363254026715, 0.8796625804067849, 0.6081907679152782, 0.10472375569094894, 0.10072161347708575, 0.2971636907461165, 0.4889005362932912, 0.3963237804099493, 0.4714064156638781, 0.6412734136583564, 0.9205428467170178, 0.26890000413415227, 0.24815040794803567, 0.2726279694159689, 0.3221539131254654, 0.10358059102957107, 0.7823087492966838, 0.8880234233563356, 0.15439418441088903, 0.7555470332269013, 0.4940939855575799, 0.31545272795705837, 0.7831402993340308, 0.3845696762223616, 0.3372679385950842, 0.3014068493636649, 0.9414203510116416, 0.5947983796465548, 0.22983364779527105, 0.7016252626379215, 0.41850971132383485, 0.7634281993299483, 0.4711481779831054, 0.811587335381992, 0.17405846471386777, 0.4297442676340072, 0.5895507755130535, 0.9281371427846702, 0.6280854169556935, 0.1779946681288357, 0.8634567469546378, 0.4717780129955157, 0.02704134075652631, 0.4335280204484564, 0.6879512741497978, 0.9791168132484419, 0.1755363367403664, 0.8791988766760731, 0.6902183807938295, 0.17931013562054066, 0.5057288504233337, 0.24105306925170655, 0.7703893042277983, 0.6432186090916264, 0.737228554207996, 0.12807136886630854, 0.11043333568185176, 0.09150332927982352, 0.6550035572893361, 0.8973340172252565, 0.9894854090636356, 0.2617522064858845, 0.19371079529149116, 0.31041726802981395, 0.20719541403745112, 0.7722248223223147, 0.4913333832205985, 0.5694946284049777, 0.22220570746958812, 0.7381026566682148, 0.6802722237174246, 0.8665721359869226, 0.6592658850268401, 0.30365722353461333, 0.8443558945567696, 0.9842507267322418, 0.5183088535179228, 0.5013460052235559, 0.520108693498454, 0.18885414550186752, 0.5187370581165175, 0.2519133479595468, 0.9723420897087393, 0.6695003643258641, 0.40796058732956386, 0.9971622192382839, 0.6146373133744784, 0.8454342199568056, 0.2915960647576987, 0.6469477300688818, 0.6993609573725049, 0.26994325555737975, 0.8197600778445524, 0.31201942125060533, 0.3100953164578808, 0.3326957071443599, 0.02087771772228919, 0.9487538760146613, 0.6330897088304498, 0.9468953881832717, 0.011526308124459539, 0.245183839243719, 0.617097863581603, 0.6137259472732226, 0.8583233683663798, 0.47943221798330615, 0.047055072914538365, 0.3383793281659967, 0.3338705592137664, 0.6353682805522649, 0.9433509936720879, 0.4763406769666647, 0.48834797516241735, 0.8257595526793472, 0.4132577165431276, 0.11051950796659826, 0.5800363272224992, 0.3451198870235618, 0.05054194955533564, 0.40312492845557624, 0.45798284971295367, 0.7763806119651566, 0.1239462154315829, 0.25307477730629246, 0.3487925850111795, 0.7301509141842174, 0.4755353011494843, 0.2970088671064186, 0.6691067431914177, 0.7409277926637637, 0.7763907637483902, 0.8394913613635152, 0.8134828923841333, 0.4778917993958294, 0.13683436403789606, 0.04235298468687876, 0.13916494308625094, 0.8100958454565816, 0.7758388087113819, 0.014324753024446024, 0.22663737050638832, 0.6853899907144513, 0.22343085032043275, 0.9440754987370219, 0.08260538135347117, 0.5871451653499218, 0.7506236057950617, 0.3800392653137378, 0.51773730973367, 0.7438710787263236, 0.7241898439518546, 0.27496223196619196, 0.8399355800369909, 0.801950810748189, 0.48199009733208753, 0.6309698971044421, 0.3080298420019417, 0.9689517153638207, 0.6170501607467425, 0.06963612397111307, 0.7811989895751197, 0.3811050002855747, 0.00447179655206198, 0.37991677248984934, 0.07351529397666046, 0.6049011519415801, 0.744077285614636, 0.6832885880763787, 0.28461262815120125, 0.298004303240357, 0.02492454792829668, 0.49678457980661506, 0.710091301642429, 0.4312547081837459, 0.3928548439824082, 0.7167433497744683, 0.997377595899509, 0.07065019656831584, 0.8870114466165427, 0.9288497285690994, 0.29775309721322096, 0.20438216670844778, 0.20032983955857386, 0.730327158806706, 0.5373488658839374, 0.4445131826121276, 0.41282817184401654, 0.8029879426757788, 0.844853122421152, 0.8701909354904136, 0.7080471892142031, 0.013534175281555871, 0.13733810478263964, 0.8444908294259355, 0.09832626052181537, 0.6863678371150512, 0.6769338117367396, 0.9978222654061405, 0.9087583178318858, 0.4429158638729286, 0.7044186041601652, 0.2489902483803318, 0.7009636098429061, 0.4199714157945744, 0.6815304928264917, 0.49435420028173205, 0.3831407372664607, 0.10064557186894496, 0.7016995965132111, 0.08779298551996573, 0.5351114885477617, 0.9521714744490671, 0.5422808951376211, 0.9166636032941649, 0.552630499018445, 0.07452131510135651, 0.3498454781104793, 0.8303459340689819, 0.47741843067284795, 0.5502054089282222, 0.4126870261341522, 0.0677564900426103, 0.01494204741399452, 0.2255309136788306, 0.39163871596267374, 0.768990061972558, 0.9080331668835959, 0.9532173114689974, 0.6647676987689309, 0.8972983320163909, 0.5493664583743584, 0.12468480744845145, 0.9219576533364646, 0.1273139700574859, 0.32400273203123986, 0.9210396848392078, 0.9037071665671433, 0.9308763099277465, 0.7833406247379479, 0.6334684344987803, 0.906185180719952, 0.07528214828400404, 0.5684504579448287, 0.021512302631849223, 0.07801068658667198, 0.789703432444829, 0.8204974981037587, 0.42321867425247317, 0.08569530979948559, 0.4744955392470048, 0.6658522195673315, 0.8294936790521967, 0.920338284544079, 0.49159328487177867, 0.05820736064451848, 0.9651145364961248, 0.3906824810641307, 0.40737457070057137, 0.9222699183206162, 0.7529234180504686, 0.3092624797813517, 0.7980024433244527, 0.5698379480024309, 0.6416658870271009, 0.05422690133366315, 0.5128394667576258, 0.8600920520637237, 0.04714654870768331, 0.027470223245544156, 0.48593425221469166, 0.08561459310699893, 0.5920366829832476, 0.6215215512042847, 0.1410290665347912, 0.8597089865088309, 0.7127638839728113, 0.9894759748840019, 0.07377245724792514, 0.3882419405346962, 0.6473572089544816, 0.3446057372789907, 0.5436836910211028, 0.6409239578605511, 0.14300460578687235, 0.40863831423187, 0.12108941787271565, 0.9002117775260451, 0.2035274969376084, 0.1238702522461399, 0.43477950340830473, 0.1781478179689684, 0.9855281078832479, 0.9487472745412457, 0.4149818985583056, 0.4406777430637263, 0.20856970187356683, 0.8596623315538781, 0.4191211062491773, 0.0765266372825758, 0.12676094679165784, 0.6082129992999072, 0.8915774621818118, 0.8186787140477747, 0.07303000395882864, 0.4772470441693334, 0.5682667228860551, 0.36843221741721044, 0.8660609289954546, 0.6433911282836816, 0.5275044501208462, 0.5676427914546717, 0.7203097969683153, 0.20223155726948427, 0.6502368371464254, 0.7588652640762518, 0.7964801680715253, 0.9150932495881898, 0.2203286419162772, 0.383694657346219, 0.41826319357894515, 0.2616918454350239, 0.7913450177344268, 0.5267239464637794, 0.1909576479683005, 0.3341628238938872, 0.812573489458132, 0.9510891315582254, 0.6499609838848919, 0.6469973792937846, 0.8273760823229974, 0.10790047136072434, 0.9288379087876455, 0.21146858760543863, 0.9826387591629651, 0.03149554872527238, 0.12037578317625619, 0.031387150873709024, 0.17058197276801723, 0.32212073733250113, 0.09337738748013658, 0.20953369397331578, 0.5244333363223111, 0.7699850854946607, 0.8794601662532361, 0.44255828237451433, 0.09009458187376118, 0.9764137093174023, 0.5944485481735599, 0.5890050216847051, 0.4570761977257314, 0.14998247889618777, 0.05626364757044999, 0.2180223714804318, 0.033000217646106944, 0.40570722472913967, 0.1055667091917083, 0.8955484483074904, 0.6576721709417748, 0.3129148471980173, 0.7091744088435082, 0.27036679695027654, 0.5545828333060318, 0.7160711431894662, 0.12515028844999576, 0.8962756769924894, 0.13853255127305253, 0.3628770918204436, 0.13922835814364976, 0.5765724864294101, 0.27127037703591117, 0.7787731466765785, 0.8294223717126933, 0.024958765914876335, 0.7903025177604142, 0.05680087363852471, 0.6336825487314713, 0.3843310758274858, 0.9433943534245696, 0.4998848843577828, 0.5872387227376085, 0.29320170960395264, 0.3280213288954934, 0.6307695701037652, 0.6056464085460072, 0.7088036716436517, 0.6194941481198429, 0.6795286326133633, 0.9039546035226349, 0.9127586230902606, 0.7606291700215555, 0.6248711808685055, 0.09380787202861729, 0.4113615648792214, 0.9217814935992579, 0.15666672565260364, 0.7712826252790871, 0.4007879755945396, 0.13902150006121983, 0.2974398274567974, 0.5776610064378231, 0.09529245146219723, 0.35121135060592057, 0.5426091222774061, 0.2200106179050526, 0.3635263976213057, 0.9085360552311676, 0.6996481448788096, 0.6948066093487576, 0.07229529457696005, 0.8156612256679912, 0.38532871674802704, 0.7529036151263263, 0.9783647654039592, 0.9347582725782047, 0.16153366022224702, 0.5581023897627212, 0.4616295865192219, 0.7532355389423839, 0.2654008897381829, 0.701175045084304, 0.29539374384186756, 0.3672284110672015, 0.08543116815098928, 0.5697674342461387, 0.30554636931737755, 0.9079526908860154, 0.7034221303376154, 0.09330322373207567, 0.8098984809490771, 0.7552625474189304, 0.1772543806706427, 0.8489734528975792, 0.981776968560794, 0.5535043246262795, 0.21166531387321008, 0.39167019895255917, 0.10566941907818617, 0.3745719164928898, 0.17375163906633762, 0.16951194285845816, 0.9342533390468736, 0.8400256306820495, 0.17419549308719773, 0.5788116942034849, 0.7717507766125565, 0.6342399024166161, 0.010097342804707976, 0.04257474359280977, 0.6331852872147457, 0.5384068762183868, 0.08592266620602129, 0.4043894354291462, 0.7357355696574865, 0.862308405536063, 0.35535162472060977, 0.0422775568706798, 0.7086448288673609, 0.06194760832918911, 0.7016120791466086, 0.5502339426261215, 0.9514750017335712, 0.7335372869456217, 0.6249074771844231, 0.030025677184995492, 0.5434856464158907, 0.27920191584428267, 0.7441707302276575, 0.39199901557970085, 0.36159636185924615, 0.4017599171458056, 0.3590736910446125, 0.6809718413911475, 0.46987506505485144, 0.8727357072240732, 0.2706173011879828, 0.7026674624677065, 0.6087717607733631, 0.3508202912871332, 0.6219520688808448, 0.42733438095310183, 0.41557942789902536, 0.9829834383900219, 0.7289384727200628, 0.6292366406648752, 0.5878542985718243, 0.7517960532672113, 0.11063036935664339, 0.7454342995120532, 0.9378412516296616, 0.4566050255137991, 0.9070228366069242, 0.02127149141185869, 0.05143590351636673, 0.5568117020247977, 0.8629844841305423, 0.2643724712237743, 0.4813593883210222, 0.8365162378067922, 0.7568437131611689, 0.030496808643555195, 0.7628631747811467, 0.19724166269639998, 0.43786094210319904, 0.6057825278170716, 0.49839011494284713, 0.8339543261995492, 0.9831383622068822, 0.7185574347431471, 0.4494330547045464, 0.7498664954225692, 0.49323997842294076, 0.9350262834549927, 0.7151852490131402, 0.2002848716030443, 0.6077641818865185, 0.4287922321191512, 0.44557432315303436, 0.7089620707537352, 0.29073662996258187, 0.8850236382847823, 0.8488182657251663, 0.8822404196814867, 0.41617321117325956, 0.5092068576626612, 0.06006856352964318, 0.826376409519469, 0.9730432224927529, 0.9693133593496318, 0.03543704686775828, 0.855679650925432, 0.3949421697202893, 0.2639506868396262, 0.32783748700520976, 0.9097196425850553, 0.45220542290056553, 0.4961263043725507, 0.5174345634099536, 0.1709358254095148, 0.8104362910528177, 0.6871847704968616], 'output': 1.0, 'delta': -0.0}]\n",
            "layer \n",
            " [{'weights': [-0.09360772115807944, -0.7027587740481428, -0.06265104909694502], 'output': 0.4811077897051657, 'delta': 0.12953785184403818}, {'weights': [-0.07239342121389307, 0.49348870647398124, 0.06265104909694508], 'output': 0.5188922102948343, 'delta': -0.12953785184403815}]\n",
            "mse of test data is 0.49936045676959306\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}